\documentclass{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{framed}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{float}

\author{Andrei Soltan\\998556067\\g0soltan@cdf.toronto.edu
\and Jonathan Prindiville\\993177628\\g2prindi@cdf.toronto.edu}
\title{CSC469: Assignment 2}
\date{17 March, 2013}

% Show paragraphs in table of contents
\setcounter{tocdepth}{5}

\lstset{
    mathescape=false,
    basicstyle=\small,
    stringstyle=\ttfamily, % typewriter type for strings
    showstringspaces=false } % no special string spaces

\begin{document}

\maketitle

\tableofcontents

\newpage
\section{Introduction}

The goal of this assignment is to implement a multithreaded memory allocator.
The challenge is to achieve an implementation that is as fast as a regular 
malloc, and whose performance scales linearly with the increased number of
threads, while at the same time avoiding false sharing and having low 
memory fragmentation. For the approach and the design of the allocator, 
We use as point of reference the Hoard allocator paper \cite{berger00}.
In addition, we have used the description and discussion of the Miser allocator
by Intel \cite{miser-intel}.

We proceed to describe the design of our allocator in section \ref{sec:design}. 
We discuss the decision points in our design and some of the alternatives we 
consided during both implementation and design in section \ref{sec:alternatives}.
Section \ref{sec:performance} discusses the performance details of the allocator
and shows the comparison with other allocators.

\section{Allocator Design}
\label{sec:design}

Our allocators design mimics that of Hoard \cite{berger00} very closely.
Because we were not permitted access to the code there were a few areas where 
we had to deduce or design our own behavior. Certain requirements of the 
assignment were influential on the design as well.

Very broadly, our allocator uses per-CPU heaps with one shared, global heap.
This is due to the expectation that threads will be bound to a single CPU,
and as a result the number of threads will be the same as the number of CPUs.
Each of those heaps has a collection of superblocks. Each superblock is of a
certain size class, meaning that it will only hold allocations of a fixed
(power of two) size. Each heap's collection of superblocks is divided into
separate lists of size classes and then further into a handful of distinct
fullness groups. As superblocks fill up or empty out they are refiled into
the appropriate fullness group.

\subsection{Metadata}
In order to manage our allocator, we store a few things globally but try to
minimize this as the assignment requires us to use less than 1k of global
storage. The bulk of the metadata is stored inside the first superblock
of space we receive from mem\_sbrk(). We assign this first superblock an
arbitrary (small) size class in the hopes that the space will be used by
calls to mm\_malloc().

Because we know that this first allocation is based at dseg\_lo we can use
some pointer arithmetic to later find our heap structures. We simply offset
from desg\_lo by the size of the superblock header and treat that address as
a heap structure pointer. Using array notation, we can access the global
heap (at index 0) and the per-CPU heaps (at indices [1, number of CPUs]).

\subsubsection{Globals}
We hold an array, size\_classes, containing at size\_classes[i] the size
in bytes of the i\textsuperscript{th} size class. The one exception is that 
size\_classes[0]=0. This is becasue, size\_class[0] is used to store empty 
superblocks which we treat as not having any size class.

We store the system's page size and our superblock size as global variables
after computing them during mm\_init().

Because calls to mem\_sbrk() must be serialized, as the assignment requires, 
we do so by using and storing a global lock, sbrk\_lock.

For our list of large allocations, we store a structure for a doubly linked 
list, to keep track of the head and tail of the list. In addition, since, for 
reasons discussed in section \ref{sec:alternatives}, the list for large
allocations is global, we maintain a lock to ensure a consistent state 
of the list at all times.

For debugging builds we chose to use pthread\_mutexes of type
PTHREAD\_MUTEX\_ERRORCHECK, so we have a global pthread\_mutexattr\_t in which
to store the type. Only enabled in such builds, we also have counters for
number of mallocs and frees.

\subsubsection{Heaps}
As mentioned above, we store our heap structures in the first superblock
that is requested from the system. Each heap structure contains the following:
\begin{description}
    \item[lock] a pthread\_mutex\_t
    \item[usage] the number of bytes currently stored in the heap
    \item[allocated] the current capacity in bytes of this heap
        (equal to the number of superblocks on the heap * superblock size)
    \item[sizes] an array containing, for each size class, pointers to
        its fullness groups: full, fullish, and emptyish.
\end{description}

\subsubsection{Superblocks}
A superblock structure is written at the header of each allocated superblock.
That header contains the following information:
\begin{description}
    \item[lock] a pthread\_mutex\_t
    \item[usage] the number of bytes currently stored in this superblock
    \item[heap] the heap number this superblock is currently associated with
    \item[size\_class] index into global size\_classes, describing the block
        size held in this superblock
    \item[prev] linked-list pointer for navigating within fullness group
    \item[next] linked-list pointer for navigating within fullness group
    \item[group] pointer to pointer to head of current fullness group
    \item[free] head of freelist linked-list (singly linked)
\end{description}

The freelist is stored in the free space of the superblock. The memory
location of a freelist node is taken to mark the head of an unallocated block.

\subsection {Behaviour}
As discussed above, our first order of business is to obtain a superblock-sized
piece of memory from the system. We write some bookkeeping structures onto it,
give it a size class which will likely be used by the calling programs and then
create a freelist in the space between our header and the end of the
superblock. This initial superblock is then put onto the global heap before we
return from mm\_init().

\subsubsection{Allocation}
Normal allocations follow much the same pattern as in Hoard. We find which of
our power of two size classes the request will fit into. If none are large 
enough, we'll take a different approach, detailed below.

Upon establishing the size class, we begin to search for a superblock with
free space in the current CPU's heap -- first in the size class, and then in the
empty, unclassified superblocks. If nothing suitable is found there, we search
the global heap for something of the correct size class and then for an empty
superblock. If we still have not found anything we'll request another
superblock's worth of memory from the system.

In any case, we take that superblock and insert it into our current CPU's heap.
This insertion puts the superblock at the head of the appropriate fullness
group's list. Head insertion is beneficial because we can do it in constant
time and also because recently accessed superblocks will remain at the front
of our lists, and hopefully stay in cache. We mimic this behaviour from Hoard.

\paragraph{Fullness groups}
Superblocks are segregated into fullness groups based on the fraction
(usage / sb\_size) as follows:
\begin{description}
    \item[full] $fullness = 1.0$
    \item[fullish] $fullness \in (1.0, SB\_EMPTYFRAC)$
	\item[emptyish] $fullness \in [SB\_EMPTYFRAC, 0]$
\end{description}

When searching non-empty superblocks, we prefer to allocate from a fullish
superblock. The intention is that those superblocks are more likely to be 
cache- and/or memory-resident.

\paragraph{Large allocation}
When the requested allocation size is larger than our size classes can 
accomodate, we classify these as large allocations and handle them separately.
We keep track of the large allocations using a doubly linked list. The linked 
list node structures are stored together with the memory segment to be returned
to the user. As such, when allocating memory we include the size of the linked
list node struct in the calculations. In addition, we require all alocations
to be aligned on superblock boundaries. This allows us to maintain consistency
in assumptions and boundary calculations between the two modes of allocations:
regular and large. Once we have determined the size of the memory segment to
allocate, we make a call to mem\_sbrk and obtain the necessary segment. After
updating the metadata (e.g. allocation size), we return the requested memory 
to the user.

\subsubsection{Freeing}
When we recieve a free request we first must determine of it is one of the
large allocations, or if it is a normal superblock-stored allocation. We
search the list of large allocations -- this is not a very fast operation,
we've chosen this method for simplicity because the benchmarks we are testing
against do not do many large allocations -- if it is not found there, we
assume that it is on a superblock. Some pointer arithmetic will find us the 
superblock's header and then we can update the heap and superblock's
statistics. We re-insert the superblock in question at the head of its
fullness group (hoping to keep it cache- and/or memory-resident) and then
check to see if we should release a superblock up to the global heap.

Large allocations require less maintenance and conditions when freeing.
Considering that our metadata contains knowledge about the the size of each 
of the large allocations, and the allocations on superblock boundaries 
allow us to easily reuse newly freed blocks from large allocations. When free 
is called on a large block, we partition it into superblocks, which are then 
initialized and added to the global heap. Due to our alignment conditions, 
this transition is possible, with relatively little processing overhead.
This reuse is meant to have a beneficial influence on memory fragmentation.

\subsubsection{Superblock release}
As in Hoard, we determine whether or not to move a superblock from the CPU's
heap up to the global heap based on the usage/allocated statistics and
two parameters: 
\begin{description}
    \item[SB\_RELTHRESHOLD] Free space on heap (measured in superblocks)
        before we consider releasing a superblock
    \item[SB\_EMPTYFRAC] Fullness percentage of heap below which we will
        release a superblock.
\end{description}

Once we determine we must release a superblock, it is chosen by the following
logic. We prefer to release from size class 0, the empty superblocks. If none
are available there, we look at each size class' emptyish list taking the
first one that we encounter.


\newpage
\section{Design Alternatives}
\label{sec:alternatives}

\subsection{Fullness groups}
In an attempt at improving memory fragmentation and usage, we have decided to
implement fullness groups. This is similar to how Hoard handles heaps that
are mostly empty. We keep track of the usage statistics similar to the Hoard
paper, which include the number of used bytes and the number of allocated bytes.
The one difference from the original Hoard is that we group the blocks into 
fullness groups, as described above. This allows for faster lookup when 
allocating and transferring memory between heaps. When allocating we select 
blocks that are moslty full, and when passing blocks to the global heap, we 
select only ones that are empty.

The memory and processing overhead are justified, since they allow for 
optimizations in several parts of the code. We benefit from this when 
allocating, freeing memory and overall in the fragmentation and usage of the 
acquired memory. The alternative would imply that often times blocks will be
underutilized, since we will not try to fill the fullest ones first. It would
also mean that empty blocks would stay in one heap where they might not be 
used for a long time. After these considerations, the implementation and 
runtime overhead are justified, especially when we reach many allocation and 
free operations.


\subsection{Large allocation}
The implementation of the large allocations was substatially different from
the Hoard paper, since using mmap() was not an option. As such, we had to
define our own design. From the beginning it was clear that large memory 
allocations are rare, hard to predict in size and frequency of occurrence, and
expensive in terms of memory and processing.

When keeping track of the large allocations, we considered replicating the
approach we use for regular superblock allocations. It soon became clear that, 
due to the unpredictable frequency and size of large allocations, it would be 
wasteful to allocate something similar to a superblock for keeping track of the
metadata of large memory segments. Instead we opted for a linked list structure
that would dynamically grow or shrink with the number of blocks used. This 
allows for the metadata space and processing cost to pe paid only when there 
is a large allocation. In considerations of space overhead it is worth 
highlighting that the large allocations are usually required when the requested 
size is more than half the size of a superblock. This puts them in the range of 
a few thousand bytes. The additional 10 bytes of metadata usually constitute an 
insignificant amount of \textless 0.25\%. As a result, the processing cost of
allocating and maintaining the list and the memory cost are amortized over the 
size of each block and over the number of large allocations made.

When deciding the placement and the visibility of the large blocks we initially
implemented them on a per heap basis. This proved to give rise to additional
challenges when a large segment allocated on CPU A, would be freed on CPU B.
This required searching through all heaps for the pointer to be freed just to 
determine if it was a pointer to a large segment. Clearly this decision would 
incur a mandatory time cost, that would be paid on every call to mm\_free. 
This time cost could be reduced if the list of large allocations were globaal, 
andr we required searching just one list. In addition, using a single global
list would reduce the cost of locking - instead of contending on a few locks 
most of the time, CPUs would contend on one lock only when making large 
allocations, which are assumed to be rare.

Freeing large allocations was another point of decision for us. Due to the
simple linked list implementation, it is sufficient to disconnect a large 
memory block from the them from the global list. This solution presents 
little challenge in terms of implementationa and processing overhead. 
However, this would mean that the large memory that was just freed is
essentailly incaccessible to any process. Thus, to avoid large unused chuncks
of memory and to improve our memory fragmentation, we have decided to 
reuse the freed large segments. This purpose implied splitting big memory 
pieces into superblocks, which in turn needed some metadata and extra 
processing. In addition, it introduced a new point of lock contention, a new 
point of failure and more bugs. So, we have decided to pay the developments 
cost, together with some price for processing, in order to improve memory
performance.

While implementing the feature of moving the superblocks from recently freed 
allocations up to the global heap, we have considered the possibility of an 
additional improvement. This option included looking up contiguous blocks
of memory to satisfy the request for large segment allocations. The rationale
behind this, is that a recently freed large allocation could be reused, instead
of requesting additional memory using mem\_sbrk. The implementation of this 
feature would required additional computations when acquiring large blocks.
Moreover, these computations are not guaranteed to provide a satisfying 
solution, which would make them detrimental to the large allocation process.
The additional implementation time, the risk of slowed allocations, and the 
possibility of additional bugs made this feature undesirable.

\newpage
\section{Performance Analysis}
\label{sec:performance}

Insert smart text about how well our allocator performs...

%%% Can move these around, just dumping them here for now %%%
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5,angle=-90]{timing-server/larson/larson.ps}
    \caption{Timing server results for 'larson' benchmark}
    \label{fig:t-larson}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5,angle=-90]{larson-greywolf/larson.ps}
    \caption{greywolf results for 'larson' benchmark}
    \label{fig:g-larson}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5,angle=-90]{timing-server/cache-thrash/cache-thrash.ps}
    \caption{Timing server results for 'cache-thrash' benchmark}
    \label{fig:t-cache-thrash}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5,angle=-90]{timing-server/cache-scratch/cache-scratch.ps}
    \caption{Timing server results for 'cache-scratch' benchmark}
    \label{fig:t-cache-scratch}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5,angle=-90]{timing-server/threadtest/threadtest.ps}
    \caption{Timing server results for 'threadtest' benchmark}
    \label{fig:t-threadtest}
\end{figure}

\textbf{MUST HAVE} or we lose marks for not mentioning (from grading rubric)
\begin{itemize}
	\item Sequential Speed -- mention that
	\item Scalability
	\item False Sharing Avoidance
	\item Fragmentation
\end{itemize}

Describe the experimental setup. (Subsection?)

\subsection{Build variations}
There are a number of parameters built in to our allocator for which we chose
values based on those discussed in the Hoard paper. We wanted to see if we
could see any patterns in the performance while varying those parameters. The
ones we chose to examine were SB\_EMPTYFRAC (emptyness of a heap required
before releasing a superblock to global heap), SB\_PAGES (size of a superblock
in pages), SB\_RELTHRESHOLD (free superblocks-worth of space on a heap required
before releasing to global heap), and NSIZES (the number of size classes
utilized -- increasing in powers of two, we get sizes \{ 0, 8, ... ,
2\^{}\textsuperscript{NSIZES+1}\}). The following table summarizes the parameters
used in the various builds.

In our build variations we observe that variations in any one parameter do not 
have a severe impact on scalability. Lowering the empty fraction or the size of 
the superblock gave similar number for scalability. However, small sizes for all
variables can have an impact both on scalability and fragmentation. We believe
this to be due to the larger number of resulting large allocations, since many 
allocations might not fit int the 1024 superblock size.

\begin{center}
    \footnotesize
    \begin{tabular}{ | l | c | c | c | c | }
    \hline
    Build name  & SB\_EMPTYFRAC & SB\_PAGES & SB\_RELTHRESHOLD & Largest size class \\
    \hline
    \hline
    libamalloc-reg  & 0.25 & 2 & 4 & 4096 \\
    \hline
    \hline
    libamalloc-lowfrac  & 0.1 & 2 & 4 & 4096 \\ \hline
    libamalloc-lowfrac-lowthresh  & 0.1 & 2 & 2 & 4096 \\ \hline
    libamalloc-lowfrac-lowthresh-smblock  & 0.1 & 2 & 2 & 1024 \\ \hline
    libamalloc-lowfrac-lowthresh-smsb  & 0.1 & 1 & 2 & 4096 \\ \hline
    libamalloc-lowfrac-lowthresh-smsb-smblock  & 0.1 & 1 & 2 & 1024 \\ \hline
    libamalloc-lowfrac-smblock  & 0.1 & 2 & 4 & 1024 \\ \hline
    libamalloc-lowfrac-smsb  & 0.1 & 1 & 4 & 4096 \\ \hline
    libamalloc-lowthresh  & 0.25 & 2 & 2 & 4096 \\ \hline
    libamalloc-lowthresh-smblock & 0.25 & 2 & 2 & 1024 \\ \hline
    libamalloc-lowthresh-smsb  & 0.25 & 1 & 2 & 4096 \\ \hline
    libamalloc-lowthresh-smsb-smblock  & 0.25 & 1 & 2 & 1024 \\ \hline
    libamalloc-smblock & 0.25 & 2 & 4 & 1024 \\ \hline
    libamalloc-smsb  & 0.25 & 1 & 4 & 4096 \\ \hline
    libamalloc-smsb-smblock  & 0.25 & 1 & 4 & 1024 \\
    \hline
\end{tabular}
\end{center}

\begin{center}
    \footnotesize
    \begin{tabular}{ | r | l | }
    \hline
    Scalability & Build name  \\
    \hline
    \input{latest-variants/allout-scal-grouped.tex}
    \hline
\end{tabular}
\end{center}

\begin{center}
    \footnotesize
    \begin{tabular}{ | r | l | }
    \hline
    Fragmentation & Build name   \\
    \hline
    \input{latest-variants/allout-frag-grouped.tex}
    \hline
\end{tabular}
\end{center}


Describe the performance measurements. \textbf{Must} have plenty of numbers 
(i.e. not "very few" according to rubric). (Subsection?)

Describe the memory usage, utilization, overhead, internal and external 
fragmentation. Pay significant attention to \textbf{fragmentation} and 
\textbf{usage}. Look at Hoard paper, performance section. 


\newpage

% Declare the bibligraphy to use one-digit indexes.
\begin{thebibliography}{9}
	
	\bibitem{berger00}
		Berger, Emery D., et al.
		"Hoard: A scalable memory allocator for multithreaded applications."
		\textit{ACM SIGPLAN Notices} 35.11 
		(2000): 
		117-128.
	\bibitem{miser-intel}
		Lewin, Stephen.
		"Miser - A Dynamically Loadable Memory Allocator for Multi-Threaded Applications."
		\textit{Intel Developer Zone.}
		Intel, 
		n.d. Web. 10 Mar. 2013.

\end{thebibliography}

\end{document}
