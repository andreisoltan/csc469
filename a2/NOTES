A compile / full test run took ~49 minutes on an empty b2200 workstation for
the three supplied allocators

# Reading: ####################################################################
-  Google's thread-safe malloc (jon: not read yet)
   http://goog-perftools.sourceforge.net/doc/tcmalloc.html
-  Doug Lea's malloc
   http://g.oswego.edu/dl/html/malloc.html
-  A Comparison of Memory Allocators in Multiprocessors (jon: not read yet)
   http://dsc.sun.com/solaris/articles/multiproc/multiproc.html
-  Miser - A Dynamically Loadable Memory Allocator for Multi-Threaded Applications
   http://software.intel.com/en-us/articles/miser-a-dynamically-loadable-memory-allocator-for-multi-threaded-applications
-  Palloc: parallel dynamic memory allocation (jon: not read yet)
   https://www.ideals.illinois.edu/handle/2142/24135

# Design notes: ###############################################################
? programmatically find page size or assume 4k?
  - can use mem_pagesize()
- Miser paper suggests redirecting requests over 256 bytes to system malloc
  implementation (most workloads do not have many of that size) not sure if
  we are allowed to do that or not. Will email Angela. [Not allowed]
- Palloc1 choses not to store metadata inside allocated pages because they
  believe allocations of exactly one page are an important use case. They
  use an internal malloc to allocate small chunks of space for metadata.
- Locking: each cpu's heap (and global) has a lock. A cpu lock must be held
  before obtaining the global heap's lock. Holders of the global heap's lock
  may not obtain any further locks.
- mem_sbrk calls must be serialized. This we can guarantee if we only call
  mem_sbrk while holding the global heap lock.


### kheap ###########################################################
- assumes 4k pages
- uses preprocessor defines SLOW and SLOWER to conditionally
compile consistency checks.
- groups allocations into size groups of powers of two:
  8, 16, 32, 64, 128, 256, 512, 1024, 2048
- structures
  - freelist
  - pageref
  - big_freelist
- guards mutiple calls to mm_init (checking for NULL == dseg_{lo,hi})
- one mutex for malloc/free.
- mm_malloc: locks, if size if more than largest subpage allocation (2048)
  does big_kmalloc, otherwise subpage_kmalloc, unlocks.
- big_kmalloc: rounds size up to page-boundary, walks through big_freelist
  selecting the first chunk that is at least as big as requested (if larger
  split it, if correct size use it). If none found, use mem_sbrk to get
  more space.
- subpage_kmalloc: figure out which size class. Look through the size class'
  list of pagerefs for one with nfree > 0. If found, * returns the first
  address from the free list, decrements free count, amends list. If not
  found, uses allocpageref to get a new page, builds a free list appropriate
  for the chunk size, gotos up to *.
- mm_free: locks, does subpage_kfree, if fails does big_kfree, unlocks.
- does not try to coalesce big chunks
- housekeeping:
  - when allocpageref gets a new page for subpage_kmalloc, it uses
  sizeof(pageref) bytes at the beginning to store the pageref struct
  - when big_kmalloc get a new chunk with mem_sbrk, it uses the first 8 bytes
  (SMALLEST_SUBPAGE_SIZE) to store the size of the chunk (number of pages)

### Hoard ###########################################################
- one global heap plus one per processor
- superblocks (some multiple of pagesize) are allocated to the per-
  processor heaps as needed. Each superblock contains chunks of a fixed
  size class. Size classes used are powers of 1.2. 
- objects larger than 1/2 superblock are mmap'ed directly -- we lose
  points if we do that.
- when the per-processor heaps cross an emptiness threshold, a 
  superblock is transferred from that heap to the global heap (they are
  usually empty at this point) -- uses "fullness groups" in order to
  find an empty(ish) superblock in constant time:

    "Each [fullness group] contains a doubly-linked list of
    superblocks that are in a given fullness range (e.g., all
    superblocks that are between 3/4 and completely empty are
    in the same bin)."
    
- "Hoard ... always allocates from nearly-full superblocks... Whenever
  we free a block ... we move the superblock to the front of its fullness
  group." Susequent allocations will likely be to a superblock that is in
  memory. Similarly, the free lists are LIFO with the goal of re-using
  blocks already in cache.
- to allocate, we check the processor's local heap for SB of the right
  size class, then for SB that is empty. If none found, check the global
  heap. If none found, then a superblock is allocated to that thread's heap.
- seems to lock on the individual heaps and the superblocks
- malloc only locks on the core-specific heap
- free locks the superblock and then the owning heap.
### CMU #############################################################
- mm_init calls mem_init and sbrks one page of memory. In the heap
  it sets up the following structure:

        dseg_lo                      dseg_hi
        |                            |
        v                            v
        | 1  | 2      | 3  ~ 4  | 5  |

        1: (4B) Pointer to head of freelist
        2: (8B) Prolog
        3: (4K - 16B) Free block, free list node is
            stored in first bytes of the block.
        4: (4B) Free block footer
        5: (4B) Epilog

- mm_malloc scans through the free list and takes the first block
  that is large enough. Splits it if it is larger updating the freelist
  if necessary (possibly including the head of the list at dseg_lo.)
- not been through mm_free

